\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}
\section{Introduction}
Turn-taking protocols are fundamental in many social aspects and activities. These protocols define who is speaking or performing a certain action in a given period of time, considering that another person or agent is waiting for that turn to complete in order to initiate their own turn. 

These protocols are different depending on the activities that are performed, for example playing a game of chess or speaking with a friend, and between the type of conversation that an agent is having, that can be dyadic or multi-party. Humans are generally very good in this type of coordination, fluently understanding who is speaking, when the next speaker should start to speak, when to stop and whom is the next turn to speak, all this with very small gaps and little overlaps. 

On the other hand different types of conversational agents different from humans, for example voice assistant or social robots, have problems managing these protocols leading to frequent misunderstandings, interruptions and long response delays. What we give for granted is a very difficult task for this type of agents, that needs some type of clues to understand when it's their turn to speak or who is the next speaker. 

As conversation system are becoming very present in ours everyday life, it is clear how important is to study and improve the turn-taking skills of these agents, with the goal to interact fluently with them as we interact with other humans. This type of fluent and not rigid interaction is fundamental to accept these type of agents, in particular social robots, as real members of ours conversations, considering that a bad or rigid interaction with them is very frustrating and that it can limit completely the conversation. 

One of the most important clues that turn-taking protocols take into consideration regarding the end of the turn of a speaking agent, is silence and in particular voice activity detection (VAD). In particular after a certain threshold period that an agent is not speaking we can derive that its turn is finished, and now another agent can take the turn. This simple idea is not so easy to implement because it is not always true. For example if we consider a pause longer than normal the listening agent could interpret that silence as a turn yielding clue and will start speaking, probably interrupting the speaker. For this reason there has been a lot of research about what are the main clues that guides turn-taking, and among the main ones we can enlist: verbal cues, prosody, breathing and gaze and gestures. Even if all the studies confirm that this clues certainly help with the turn-taking decision, that is the decision about if the current turn is ended, all researches agrees that the main clue remain the silence after the end of a speaking unit.

The goal of this thesis is to develop a remote real-time noise robust turn-taking management system, based on voice activity detection using a neural voice activity detector, in order to use this module inside the Abel android. 

Abel is a new generation hyperrealistic humanoid robot, conceived to be a research platform for social interaction, emotion modeling, and studies on embodied intelligence. Its appearance resembles that of an 11â€“12 year old boy. It is a unique piece, resulting from the collaboration between the Enrico Piaggio Research Center of the University of Pisa and Gustav Hoegen, from Biomimic Studio, based in London. Abel is physically made up of the head and the upper part of the torso with arms and hands, all of these are robotic parts moved by the latest generation of servo motors. The mechatronics of the head confers to Abel the possibility to express a wide spectrum of emotions by means of facial expressions, which are accompanied by a body which is also designed to represent emotional and meaningful body gestures.
The humanoid is equipped with an integrated camera into the torso and integrated binaural microphones, which are specifically designed to emulate the acoustic perception of the human listener. The robot also has an internal speaker to reproduce its voice. Therefore, Abel represents an incredible robotic platform to implement and test theories coming
from neuroscience, psychology and sociology, with very promising applications in therapy and
diagnosis of mental illness, learning disabilities, autism spectrum, and dementia.
It also generally gives us the opportunity to embed ideally every capability given by Artificial
Intelligence in a human-like body capable of expressing and estimating the emotions of its human
interlocutors. In Abel the voice acquired by the microphone is translated to text by a speech-to-text neural model. This text is then passed to a generative neural model that generates a response to the voice input, based on a certain context.  

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/Abel.jpg}
    \caption{Abel}
    \label{fig:abel}
\end{figure}

One of the main issue that Abel was having, was regarding the turn-taking module and in particular regarding the understanding of the end of the speaking agent's turn. This was because the end of turn detection was based on the end of the sound perceived by the robot's microphones. This technique wasn't very effective because in presence of noise, Abel wasn't able to distinguish noise from speaker words. This, lead to a phenomenon where Abel didn't detect the change of turn when a speaker ended his turn, because of a background noise, leading to very long response delays that made the conversation unrealistic and complicated. 

So the idea was to upgrade the turn-taking module of Abel using a neural voice activity detector, to detect the turn change in order to avoid that casual background noise would affect the conversation; leading to a more realistic and fluid interaction. In particular the voice activity detector chosen for this application has been Silero VAD. The model was used through the well known PyTorch library to perform the inference. In order to minimize the latency of the model it is better to use an enough powerful CPU, to perform the inference of the model given an audio chunk. Given that the architecture present in local, where Abel is placed is not so powerful, we wanted to implement an architecture that allowed us to perform the task of remote real-time voice activity detection, in order to deploy the module on a more powerful remote machine, where also the other components of the Abel architecture are deployed. In order to achieve this, it has been developed a software architecture using the WebRTC protocol, to send in real time the voice acquired from the Abel's microphones to the remote server that processes the audio and performs the voice activity detection with the Silero VAD model. It is important to underline how the real time feature of the audio transmission is fundamental because of the low latency requirements required by the use case application. It has been implemented also a web-socket capability in the module to communicate to the others components of Abel the current status of the turn-taking protocol, and the current situation of the conversation.

Furthermore, we validated the performances of the system both in terms of end of turn detection latency and in terms of accuracy in the voice activity detection task. To do that we created an ad hoc dataset with different categories of noise and different Signal to Noise Ratio (SNR), that is the level of noise with respect to the level of speech. We also compared the results of the system using the Silero VAD model and the WebRTC vad. 

The thesis is organized as follows:
In section 2 "Literature review", has been exposed a literature review about the main methods used in turn-taking protocols for human robot interaction and a list of the main models and features used in voice activity detection tasks. In section 3 "Methods and methodology", there is a comprehensive description of the tool used and the motivation behind their chooses. In section 4 "Implementation", will be presented the detailed implementation of the software architecture. Section 5 "Experiments and results", will present an analysis of the performances of the system about the latency of the system in terms of real time audio communication and model inference, and in terms of accuracy in the VAD task. Finally, in section 6 "Conclusion and future works" are presented the conclusions obtained from this work and the possible future extension to the system, in order to achieve better performance or further capabilities.

At this link: \url{https://github.com/EhiSuper/TurnTakingThesis}, it is possible to find the complete code of this thesis and also the documentation files.

\end{document}
